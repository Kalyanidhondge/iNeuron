{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53028db",
   "metadata": {},
   "source": [
    "# cv_assignment - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93684c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAn inception network is a deep neural network with an architectural design that consists of repeating components referred to as Inception modules. \\nInception Modules are used in Convolutional Neural Networks to allow for more efficient computation and deeper Networks through a dimensionality reduction with stacked 1×1 convolutions.\\nInception does a 5×5 convolutional transformation, and a 3×3, and a max-pool.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "An inception network is a deep neural network with an architectural design that consists of repeating components referred to as Inception modules. \n",
    "Inception Modules are used in Convolutional Neural Networks to allow for more efficient computation and deeper Networks through a dimensionality reduction with stacked 1×1 convolutions.\n",
    "Inception does a 5×5 convolutional transformation, and a 3×3, and a max-pool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a9b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAn Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN.\\nit allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block,\\nwhich we then concatenate and pass onto the next layer.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN.\n",
    "it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block,\n",
    "which we then concatenate and pass onto the next layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bfd76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDimensionality reduction refers to techniques for reducing the number of input variables in training data.\\n\\n>> a 1×1 convolutional layer can be used that offers a channel-wise pooling, often called feature map pooling or a projection layer.\\nThis simple technique can be used for dimensionality reduction, decreasing the number of feature maps whilst retaining their salient features.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "Dimensionality reduction refers to techniques for reducing the number of input variables in training data.\n",
    "\n",
    ">> a 1×1 convolutional layer can be used that offers a channel-wise pooling, often called feature map pooling or a projection layer.\n",
    "This simple technique can be used for dimensionality reduction, decreasing the number of feature maps whilst retaining their salient features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e85a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDimension reduction chooses the attributes of a mission in the dataset before dataset knowledge exploring. \\nThe analysis of the dataset could be misleading and more complex because of the redundant and not useful attributes in the procedures of data mining which makes the results inaccurate\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "Dimension reduction chooses the attributes of a mission in the dataset before dataset knowledge exploring. \n",
    "The analysis of the dataset could be misleading and more complex because of the redundant and not useful attributes in the procedures of data mining which makes the results inaccurate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f23599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nan input layer, an output layer, and one or more hidden layers\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    "an input layer, an output layer, and one or more hidden layers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d163a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResNet is an ANN, It is a gateless or open-gated variant of the HighwayNet,\\nthe first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks\\n\\n>> There is a 34-layer plain network in the architecture that is inspired by VGG-19 in which the shortcut connection or the skip connections are added. \\nThese skip connections or the residual blocks then convert the architecture into the residual network\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "ResNet is an ANN, It is a gateless or open-gated variant of the HighwayNet,\n",
    "the first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks\n",
    "\n",
    ">> There is a 34-layer plain network in the architecture that is inspired by VGG-19 in which the shortcut connection or the skip connections are added. \n",
    "These skip connections or the residual blocks then convert the architecture into the residual network\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02661fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSkip Connections skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    "Skip Connections skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "069b8520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "A residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbce2a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nit works by transferring as much knowledge as possible from an existing model to a new model designed for a similar task.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "it works by transferring as much knowledge as possible from an existing model to a new model designed for a similar task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "785c6bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntransfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task.\\na model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q10\n",
    "\"\"\"\n",
    "transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task.\n",
    "a model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c88eb4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNeural networks work by propagating forward inputs, weights and biases.\\nHowever, it's the reverse process of backpropagation where the network actually learns by determining \\nthe exact changes to make to weights and biases to produce an accurate result.\\n\\n>> CNN uses learned filters to convolve the feature maps from the previous layer. \\nFilters are two- dimensional weights and these weights have a spatial relationship with each other.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q11\n",
    "\"\"\"\n",
    "Neural networks work by propagating forward inputs, weights and biases.\n",
    "However, it's the reverse process of backpropagation where the network actually learns by determining \n",
    "the exact changes to make to weights and biases to produce an accurate result.\n",
    "\n",
    ">> CNN uses learned filters to convolve the feature maps from the previous layer. \n",
    "Filters are two- dimensional weights and these weights have a spatial relationship with each other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dad3471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning and Fine-tuning are used interchangeably and are defined as the process of training a neural network on new data but initialising it with pre-trained weights obtained from training it on a different,\\nmostly much larger dataset, for a new task which is somewhat related to the data and task the network\\nFine tuning is like optimization. We optimize the network to achieve the optimal results. \\nMay be we can change the number of layers used, no of filters, learning rate and we have many parameters of the model to optimize.\\nso fine tuning is better than start-up training\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q12\n",
    "\"\"\"\n",
    "Learning and Fine-tuning are used interchangeably and are defined as the process of training a neural network on new data but initialising it with pre-trained weights obtained from training it on a different,\n",
    "mostly much larger dataset, for a new task which is somewhat related to the data and task the network\n",
    "Fine tuning is like optimization. We optimize the network to achieve the optimal results. \n",
    "May be we can change the number of layers used, no of filters, learning rate and we have many parameters of the model to optimize.\n",
    "so fine tuning is better than start-up training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65365c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
