{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faea8de5",
   "metadata": {},
   "source": [
    "# cv_assignment - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736709c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe number of hidden neurons should be between the size of the input layer and the size of the output layer.\\n\\n>> The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numer of hidden layers :\n",
    "\"\"\"\n",
    "The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "\n",
    ">> The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd31baf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA network architecture defines the way in which a deep learning model is structured and more importantly what it's designed to do.\\nThe architecture will determine: The model's accuracy (a network architecture is one of many factors that impacts accuracy) What the model can predict.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Network architecture (network depth):\n",
    "\"\"\"\n",
    "A network architecture defines the way in which a deep learning model is structured and more importantly what it's designed to do.\n",
    "The architecture will determine: The model's accuracy (a network architecture is one of many factors that impacts accuracy) What the model can predict.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b605b823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthe first hidden layer will have hidden layer neurons equal to the number of lines, the first hidden layer will have four neurons. \\nIn other words, there are four classifiers each created by a single layer perceptron. At the current time, the network will generate four outputs, one from each classifier\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each layer's number of neurons:\n",
    "\"\"\"\n",
    "the first hidden layer will have hidden layer neurons equal to the number of lines, the first hidden layer will have four neurons. \n",
    "In other words, there are four classifiers each created by a single layer perceptron. At the current time, the network will generate four outputs, one from each classifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c72a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Activation functions can be divided into three types:\\nLinear Activation Function, \\nBinary Step Function and \\nNon-linear Activation Functions'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Form of activation in deep learning:\n",
    "\"\"\"Activation functions can be divided into three types:\n",
    "Linear Activation Function, \n",
    "Binary Step Function and \n",
    "Non-linear Activation Functions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e68fe25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn optimization, we care only about the data in hand. We know that finding the maximum value will be the best solution to our problem. In Learning, we mostly care about generalization i.e the data we don't have\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimization and learning:\n",
    "\"\"\"\n",
    "In optimization, we care only about the data in hand. We know that finding the maximum value will be the best solution to our problem. \\\n",
    "In Learning, we mostly care about generalization i.e the data we don't have\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e8567b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe learning rate is a parameter that determines how much an updating step influences the current value of the weights.\\nWhile decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Learning rate and decay schedule:\n",
    "\"\"\"\n",
    "The learning rate is a parameter that determines how much an updating step influences the current value of the weights.\n",
    "While decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881b2ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMini-batch sizes, commonly called “batch sizes” for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mini batch size:\n",
    "\"\"\"\n",
    "Mini-batch sizes, commonly called “batch sizes” for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992d5d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient Descent,\\nMomentum,\\nAdagrad,\\nRMSProp,\\nAdam\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Algorithms for optimization:\n",
    "\"\"\"\n",
    "Gradient Descent,\n",
    "Momentum,\n",
    "Adagrad,\n",
    "RMSProp,\n",
    "Adam\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a8bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWithout early stopping, the model runs for all 50 epochs and we get a validation accuracy of 88.8%, \\nwith early stopping this runs for 15 epochs and the test set accuracy is 88.1%.\\n>> the optimal number of epochs to train most dataset is 11\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of epochs (and early stopping criteria) :\n",
    "\"\"\"\n",
    "Without early stopping, the model runs for all 50 epochs and we get a validation accuracy of 88.8%, \n",
    "with early stopping this runs for 15 epochs and the test set accuracy is 88.1%.\n",
    ">> the optimal number of epochs to train most dataset is 11\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc14bf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe simplest way to reduce overfitting is to increase the data, and this technique helps in doing so. \\nData augmentation is a regularization technique, which is used generally when we have images as data sets\\nl1 regularization / Lasso\\nl2 regularization /Ridge\\nDropout\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overfitting that be avoided by using regularization techniques :\n",
    "\"\"\"\n",
    "The simplest way to reduce overfitting is to increase the data, and this technique helps in doing so. \n",
    "Data augmentation is a regularization technique, which is used generally when we have images as data sets\n",
    "l1 regularization / Lasso\n",
    "l2 regularization /Ridge\n",
    "Dropout\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22c59448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt may be defined as the normalization technique that modifies the dataset values in a way \\nthat in each row the sum of the squares will always be up to 1. It is also called least squares.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#l2 normalization\n",
    "\"\"\"\n",
    "It may be defined as the normalization technique that modifies the dataset values in a way \n",
    "that in each row the sum of the squares will always be up to 1. It is also called least squares.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31233db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Dropout layer randomly sets input units to 0 with a frequency of rate at each \\nstep during training time,which helps prevent overfitting.\\n>> dropout is placed on the fully connected layers \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop out layers\n",
    "\"\"\"\n",
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each \n",
    "step during training time,which helps prevent overfitting.\n",
    ">> dropout is placed on the fully connected layers \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "168daaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData augmentation is a process of artificially increasing the amount of data by generating new data points from existing data.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data augmentation\n",
    "\"\"\"\n",
    "Data augmentation is a process of artificially increasing the amount of data by generating new data points from existing data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
